{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def kelas (file):\n",
    "#         if 'ha' in file: dot=1\n",
    "#         if 'na' in file: dot=2\n",
    "#         if 'ca' in file: dot=3\n",
    "#         if 'ra' in file: dot=4\n",
    "#         if 'ka' in file: dot=5\n",
    "#         if 'da' in file: dot=6\n",
    "#         if 'ta' in file: dot=7\n",
    "#         if 'sa' in file: dot=8\n",
    "#         if 'wa' in file: dot=9\n",
    "#         if 'la' in file: dot=10\n",
    "#         if 'ma' in file: dot=11\n",
    "#         if 'ga' in file: dot=12\n",
    "#         if 'ba' in file: dot=13\n",
    "#         if 'nga' in file: dot=14\n",
    "#         if 'pa' in file: dot=15\n",
    "#         if 'ja' in file: dot=16\n",
    "#         if 'ya' in file: dot=17\n",
    "#         if 'nya' in file: dot=18\n",
    "#         return (dot-1)\n",
    "\n",
    "#     def minmax (img):#normalisasi min max\n",
    "#         x,y = img.shape\n",
    "#         balik = np.zeros(img.shape)\n",
    "#         maks  = max(img.flatten())\n",
    "#         mins = min(img.flatten())\n",
    "#         for i in range(x):\n",
    "#             for j in range (y):\n",
    "#                 balik[i,j]=(img[i,j]-mins)/(maks-mins)\n",
    "#         return balik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"D:\\\\cross\"\n",
    "\n",
    "traindata = []\n",
    "trainlabel =[]\n",
    "testdata =[]\n",
    "testlabel =[]\n",
    "for a in os.listdir(filepath):\n",
    "    c = 1\n",
    "    for b in os.listdir(filepath + \"\\\\\" + str(a)):\n",
    "        for nama in os.listdir(filepath + \"\\\\\" + str(a) + \"\\\\\" + str(b)):\n",
    "            if (int(a)<=2):\n",
    "                img= cv2.imread(filepath + \"\\\\\" + str(a) + \"\\\\\" + str(b)+ \"\\\\\" + str(nama),0)#grayscale\n",
    "                x = cv2.resize(img,(64,64))#resize\n",
    "                x = minmax(x)\n",
    "                testdata.append(x)\n",
    "                testlabel.append(kelas (nama))\n",
    "            else :\n",
    "                img= cv2.imread(filepath + \"\\\\\" + str(a) + \"\\\\\" + str(b)+ \"\\\\\" + str(nama),0)#grayscale\n",
    "                x = cv2.resize(img,(64,64))#resize\n",
    "                x = minmax(x)\n",
    "                traindata.append(x)\n",
    "                trainlabel.append(kelas (nama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testlabel= np.array(testlabel)\n",
    "# trainlabel = np.array(trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(dt).shape)\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x1,x2,y1,y2= train_test_split(dt,ID,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain = np.array(x1)\n",
    "# dlabel = np.array(y1)\n",
    "# ttrain = np.array(x2)\n",
    "# tlabel = np.array(y2)\n",
    "# print(dtrain.shape, ttrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdtrain = np.zeros ((8640,64,64,1))\n",
    "# fdtrain[:,:,:,0] = traindata\n",
    "# fttrain = np.zeros ((2160,64,64,1))\n",
    "# fttrain[:,:,:,0] = testdata\n",
    "# fdtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    rotation_range = 20,\n",
    "                    horizontal_flip = False,\n",
    "                    \n",
    "                    fill_mode = 'wrap',\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8640 images belonging to 18 classes.\n",
      "Found 2160 images belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "fpath = \"D:\\\\data\"\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                  fpath,\n",
    "                  color_mode='grayscale',\n",
    "                  target_size = (128,128),\n",
    "                  subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "                  fpath,\n",
    "                  color_mode='grayscale',\n",
    "                  target_size = (128,128),\n",
    "                  subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(train_generator).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(128, (5, 5), activation='relu',padding='same', input_shape=(128,128,1)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (5, 5),padding='same', activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (5, 5),padding='same', activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(18, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_75 (Conv2D)           (None, 128, 128, 128)     3328      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 64, 64, 128)       409728    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 32, 32, 128)       409728    \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 128)               16777344  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 18)                2322      \n",
      "=================================================================\n",
      "Total params: 17,602,450\n",
      "Trainable params: 17,602,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import Callback\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "# class Metrics(Callback):\n",
    "    \n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.val_accuracy = []\n",
    "#         self.val_recalls = []\n",
    "\n",
    "#         self.val_precisions = []\n",
    "        \n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         pass\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         val_predict = model.predict_classes(validation_generator, verbose=0)\n",
    "#         val_targ = validation_generator.class_indices\n",
    "        \n",
    "#         _val_recall = recall_score(val_targ, val_predict, average='weighted')\n",
    "#         _val_precision = precision_score(val_targ, val_predict, average='weighted')\n",
    "#         _val_accuracy = accuracy_score(val_targ, val_predict)\n",
    "        \n",
    "#         self.val_recalls.append(_val_recall)\n",
    "#         self.val_precisions.append(_val_precision)\n",
    "#         self.val_accuracy.append(_val_accuracy)\n",
    "        \n",
    "#         print (\"\\n\\n * val_accuracy: %f — val_precision: %f — val_recall %f\" %(_val_accuracy, _val_precision, _val_recall), \"\\n \\n\")\n",
    "#         return\n",
    "    \n",
    "    \n",
    "# met = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "MemoryError: Unable to allocate 2.00 MiB for an array with shape (32, 128, 128, 1) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 611, in generator\n    yield x[i]\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nMemoryError: Unable to allocate 2.00 MiB for an array with shape (32, 128, 128, 1) and data type float32\n\n\n\t [[{{node PyFunc}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-600d0004522a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#batch_size=60,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"waktu training adalah : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmulai\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\" s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    608\u001b[0m   \u001b[1;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_ADAPTER_FOR_STANDARDIZE_USER_DATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[0mtraining_v2_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_prepare_model_with_inputs\u001b[1;34m(model, dataset)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_input_from_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_get_input_from_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_get_input_from_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m   \u001b[1;34m\"\"\"Get elements from the iterator and verify the input shape and type.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m   \u001b[0mnext_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m   if (tensor_util.is_tensor(next_element) or\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2670\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2672\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\flow\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: MemoryError: Unable to allocate 2.00 MiB for an array with shape (32, 128, 128, 1) and data type float32\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\", line 611, in generator\n    yield x[i]\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 65, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"C:\\Users\\Coild\\anaconda3\\envs\\flow\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 222, in _get_batches_of_transformed_samples\n    batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=self.dtype)\n\nMemoryError: Unable to allocate 2.00 MiB for an array with shape (32, 128, 128, 1) and data type float32\n\n\n\t [[{{node PyFunc}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy', tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "mulai = time.time()\n",
    "history = model.fit(train_generator,\n",
    "    steps_per_epoch = 100,      \n",
    "    #batch_size=60,\n",
    "    epochs = 100,\n",
    "    validation_data = validation_generator\n",
    "    )\n",
    "print(\"waktu training adalah : \", time.time()-mulai , \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "# plt.plot(np.array(history.history['val_recall_8']), label = 'val_recall')\n",
    "# plt.plot(np.array(history.history['val_precision_9']), label = 'val_precission')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.5, 1.1])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# test_loss, test_acc = model.evaluate(validation_generator,  validation_generator.label, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.version.VERSION)\n",
    "\n",
    "# model_path = '../out/modeltf.h5'\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# tflite_model = converter.convert()\n",
    "# open(\"../out/convert_try.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # yhat_classes = model.predict_classes(fttrain, verbose=0)\n",
    "# # yhat_classes\n",
    "# new_model = tf.keras.models.load_model(model_path)\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model.predict(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# y_pred=model.predict_classes(fttrain)\n",
    "# con_mat = tf.math.confusion_matrix(labels=tlabel, predictions=y_pred).numpy()\n",
    "# con_mat[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import precision_score\n",
    "# from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # accuracy: (tp + tn) / (p + n)\n",
    "# accuracy = accuracy_score(tlabel, yhat_classes)\n",
    "# print('Accuracy: %f' % accuracy)\n",
    "# # precision tp / (tp + fp)\n",
    "# precision = precision_score(tlabel, yhat_classes, average='weighted')\n",
    "# print('Precision: %f' % precision)\n",
    "# # recall: tp / (tp + fn)\n",
    "# recall = recall_score(tlabel, yhat_classes, average='weighted')\n",
    "# print('Recall: %f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
